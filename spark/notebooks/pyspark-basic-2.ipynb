{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3일차 2교시 기본 연산 다루기\n",
    "> 스파크의 \"기본 연산자\" 와 \"데이터프레임\"에 대해 학습합니다\n",
    "\n",
    "## 목차\n",
    "* [1. 기본 연산자](#1.-기본-연산자)\n",
    "  - [1.1 데이터 프레임 함수](#1.1-데이터-프레임-함수)\n",
    "  - [1.2 컬럼 함수](#1.2-컬럼-함수)\n",
    "  - [1.3 기타 함수](#1.3-기타-함수)\n",
    "* [2. RDD 의 특징](#3.-RDD-의-특징)\n",
    "  - [2.1 RDD 통한 데이터 변환](#3.1-RDD-통한-데이터-변환)\n",
    "  - [2.2 구조화 API 통한 데이터 변환](#3.2-구조화-API-통한-데이터-변환)\n",
    "* [3. 데이터 타입](#4.-데이터-타입)\n",
    "* [4. 핵심 데이터 프레임 연산자](#2.-핵심-데이터-프레임-연산자)\n",
    "  - [4.1 파일로 부터 테이블 만들어 사용하기](#2.1-파일로-부터-테이블-만들어-사용하기)\n",
    "  - [4.2 특정 컬럼 선택 (select, selectExpr)](#2.2-특정-컬럼-선택-(select,-selectExpr))\n",
    "  - [4.3 상수값 사용하기](#2.3-상수값-사용하기)\n",
    "  - [4.4 컬럼 추가하기](#2.4-컬럼-추가하기)\n",
    "  - [4.5 컬럼명 바꾸기](#2.5-컬럼명-바꾸기)\n",
    "  - [4.6 컬럼 제거하기](#2.6-컬럼-제거하기)\n",
    "  - [4.7 컬럼의 데이터 타입 변경하기](#2.7-컬럼의-데이터-타입-변경하기)\n",
    "  - [4.8 레코드 필터링](#2.8-레코드-필터링)\n",
    "  - [4.9 유일값 (DISTINCT)](#2-9-유일값-(DISTINCT))\n",
    "  - [4.10 정렬 (SORT)](#2.10-정렬-(SORT))\n",
    "  - [4.11 로우 수 제한 (LIMIT)](#2.11-로우-수-제한-(LIMIT))\n",
    "* [5. 기타 데이터 프레임 연산자](#5.-기타-데이터-프레임-연산자)\n",
    "  - [5.1 사전에 스키마를 정의하는 장점](#5.1-사전에-스키마를-정의하는-장점)\n",
    "  - [5.2 스키마를 정의하는 두 가지 방법](#5.2-스키마를-정의하는-두-가지-방법)\n",
    "  - [5.3 중첩된 배열 스키마](#5.3-중첩된-배열-스키마)\n",
    "  - [5.4 컬럼과 표현식](#5.4-컬럼과-표현식)\n",
    "  - [5.5 로우 생성 및 다루기](#5.5-로우-생성-및-다루기)\n",
    "  - [5.6 파케이 파일 혹은 테이블 저장](#5.6-파케이-파일-혹은-테이블-저장)\n",
    "  - [5.7 프로젝션과 필터](#5.7-프로젝션과-필터)\n",
    "  - [5.8 날짜 관련 함수](#5.8-날짜-관련-함수)\n",
    "* [6. 데이터셋 API](#6.-데이터셋-API)\n",
    "  - [6.1 데이터셋과 데이터프레임 비교](#6.1-데이터셋과-데이터프레임-비교)\n",
    "  - [6.2 데이터셋 데이터프레임 그리고 RDD](#6.2-데이터셋-데이터프레임-그리고-RDD)\n",
    "* [7. 카탈리스트 옵티마이저](#7.-카탈리스트-옵티마이저)\n",
    "  - [7.1 분석 (Analysis)](#7.1-분석-(Analysis))\n",
    "  - [7.2 논리 최적화 (Logical Optimization)](#7.2-논리-최적화-(Logical-Optimization))\n",
    "  - [7.3 물리 계획 (Physical Planning)](#7.3-물리-계획-(Physical-Planning))\n",
    "  - [7.4 코드 생성 (Code Generation)](#7.4-코드-생성-(Code-Generation))\n",
    "* [8. 실습 문제](#8.-실습-문제)\n",
    "* 참고 링크\n",
    "  - [PySpark SQL Modules Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기본 연산자\n",
    "---\n",
    "### 1.1 데이터 프레임 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.printSchema() | 스키마 정보를 출력합니다. | - |\n",
    "| df.schema | StructType 스키마를 반환합니다 | - |\n",
    "| df.columns | 컬럼명 정보를 반환합니다 | - |\n",
    "| df.show(n) | 데이터 n 개를 출력합니다 | - |\n",
    "| df.first() | 데이터 프레임의 첫 번째 Row 를 반환합니다 | - |\n",
    "| df.head(n) | 데이터 프레임의 처음부터 n 개의 Row 를 반환합니다 | - |\n",
    "| df.createOrReplaceTempView | 임시 뷰 테이블을 생성합니다 | - |\n",
    "| df.union(newdf) | 데이터프레임 간의 유니온 연산을 수행합니다 | - |\n",
    "| df.limit(n) | 추출할 로우수 제한 | T |\n",
    "| df.repartition(n) | 파티션 재분배, 셔플발생 | - |\n",
    "| df.coalesce() | 셔플하지 않고 파티션을 병합 | 마지막 스테이지의 reduce 수가 줄어드는 효과로 성능저하에 유의해야 합니다 |\n",
    "| df.collect() | 모든 데이터 수집, 반환 | A |\n",
    "| df.take(n) | 상위 n개 로우 반환 | A |\n",
    "\n",
    "---\n",
    "### 1.2 컬럼 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.select | 컬럼이나 표현식 사용  | - |\n",
    "| df.selectExpr | 문자열 표현식 사용 = df.select(expr()) | - |\n",
    "| df.withColumn(컬럼명, 표현식) | 컬럼 추가, 비교, 컬럼명 변경 | - |\n",
    "| df.withColumnRenamed(old_name, new_name) | 컬럼명 변경 | - |\n",
    "| df.drop() | 컬럼 삭제 | - |\n",
    "| df.where | 로우 필터링 | - |\n",
    "| df.filter | 로우 필터링 | - |\n",
    "| df.sort, df.orderBy | 정렬 | - |\n",
    "| df.sortWithinPartitions | 파티션별 정렬 | - |\n",
    "\n",
    "---\n",
    "### 1.3 기타 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| expr(\"someCol - 5\") | 표현식 | - |\n",
    "| lit() | 리터럴 | - |\n",
    "| cast() | 컬럼 데이터 타입 변경 | - |\n",
    "| distinct() | unique row | - |\n",
    "| desc(), asc() | 정렬 순서 | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RDD 의 특징\n",
    "\n",
    "| 특징 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| dependencies | resilency | 리니지를 통해 의존성 정보를 유지함으로써 언제든 다시 수행할 수 있는 회복력을 가집니다 |\n",
    "| partitions | parallelize computation | 파티션 단위로 데이터를 저장 관리하므로써 병렬 처리를 가능하게 합니다 |\n",
    "| compute function | Iterator\\[T\\] | RDD로 저장되는 모든 데이터는 반복자를 통해 함수를 적용할 수 있습니다 |\n",
    "\n",
    "* 반면에 compute function 의 내부를 spark 가 알 수 없기 때문에 오류를 찾아내가 어려우며, Python 과 같은 스크립트 언어는 generic object 로만 인식이 되므로 호환하기 어려우며, T 타입의 객체는 직렬화되어 전달되기만 할 뿐 스파크는 해당 데이터 타입 T 에 대해 알 수 없습니다\n",
    "\n",
    "> RDD 를 통해 데이터 처리하는 방법과, 구조화된 API 를 통해 처리하는 방법을 비교해 보고, 이러한 고수준의 DSL 연산자를 통해 보다 단순하게 표현이 가능합니다.\n",
    "\n",
    "### 2.1 RDD 통한 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Name| Age|\n",
      "+------+----+\n",
      "|   Cat|27.0|\n",
      "|   Dog|19.0|\n",
      "|Monkey|28.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataRDD = spark.sparkContext.parallelize([(\"Cat\", 30), (\"Dog\", 28), (\"Monkey\", 28), (\"Cat\", 24), (\"Dog\", 10)])\n",
    "agesRDD = dataRDD.map(lambda x: (x[0], (x[1], 1))) \\\n",
    "     .reduceByKey(lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1])) \\\n",
    "     .map(lambda v: (v[0], v[1][0]/v[1][1]))\n",
    "agesRDD.toDF([\"Name\", \"Age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 구조화 API 통한 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|Name  |Age |\n",
      "+------+----+\n",
      "|Cat   |27.0|\n",
      "|Monkey|28.0|\n",
      "|Dog   |19.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"동물의 평균 수명\").getOrCreate()\n",
    "animal = spark.createDataFrame([(\"Cat\", 30), (\"Dog\", 28), (\"Monkey\", 28), (\"Cat\", 24), (\"Dog\", 10)], [\"Name\", \"Age\"])\n",
    "ages = animal.select(\"Name\", \"Age\").groupBy(\"Name\").agg(avg(\"Age\").alias(\"Age\"))\n",
    "ages.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 타입\n",
    "> immutable 하며, 모든 transformation 들의 lineage 를 유지합니다. 또한 컬럼을 변경, 추가 등을 통해 새로운 데이터프레임을 생성합니다.\n",
    "\n",
    "| python | scala |\n",
    "|---|---|\n",
    "| ![python](images/datatypes-python.png) | ![scala](images/datatypes-scala.png) | \n",
    "| ![python](images/datatypes-python2.png) | ![scala](images/datatypes-scala2.png) | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 핵심 데이터 프레임 연산자\n",
    "\n",
    "### 4.1 파일로 부터 테이블 만들어 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 원시 데이터로 부터 읽거나, Spark SQL 통한 결과는 항상 데이터프레임이 생성됩니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 원시 데이터로 부터 읽거나, Spark SQL 통한 결과는 항상 데이터프레임이 생성됩니다\")\n",
    "df = spark.read.json(\"data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"2015_summary\")\n",
    "\n",
    "sql_result = spark.sql(\"SELECT * FROM 2015_summary\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 특정 컬럼 선택 (select, selectExpr)\n",
    "> 아래의 모든 예제에서 컬럼 선택 시에 select(col(\"컬럼명\")) 으로 접근할 수도 있지만 **selectExpr(\"컬럼명\") 이 간결하기 때문에 앞으로는 가능한 표현식으로 사용**하겠습니다 <br>\n",
    "컬럼 표현식의 경우 반드시 하나의 컬럼은 하나씩 표현되어야만 합니다.  <br>\n",
    "잘된예 : \"컬럼1\", \"컬럼2\" <br>\n",
    "잘못된예: \"컬럼1, 컬럼2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# select 표현은 컬럼만 입력이 가능하며, 함수나 기타 표현식을 사용할 수 없습니다. 사용하기 위해서는 functions 를 임포트 하고, 개별 함수의 특징을 잘 이해하고 사용해야 합니다\n",
      "+------------------------+-------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|ORIGIN_COUNTRY_NAME|\n",
      "+------------------------+-------------------+\n",
      "|           UNITED STATES|            Romania|\n",
      "|           UNITED STATES|            Croatia|\n",
      "+------------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "# selectExpr 별도의 임포트 없이, 모든 표현식을 사용할 수 있습니다\n",
      "+------------------------+-------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|ORIGIN_COUNTRY_NAME|\n",
      "+------------------------+-------------------+\n",
      "|           UNITED STATES|            Romania|\n",
      "|           UNITED STATES|            Croatia|\n",
      "+------------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"# select 표현은 컬럼만 입력이 가능하며, 함수나 기타 표현식을 사용할 수 없습니다. 사용하기 위해서는 functions 를 임포트 하고, 개별 함수의 특징을 잘 이해하고 사용해야 합니다\")\n",
    "df.select(upper(col(\"DEST_COUNTRY_NAME\")), \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "print(\"# selectExpr 별도의 임포트 없이, 모든 표현식을 사용할 수 있습니다\")\n",
    "df.selectExpr(\"upper(DEST_COUNTRY_NAME)\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 앨리어스 혹은 전체 컬럼을 위한 * 도 사용할 수 있습니다\n",
      "+-------------+-----------------+\n",
      "| newColmnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 앨리어스 혹은 전체 컬럼을 위한 * 도 사용할 수 있습니다\")\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColmnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 상수값 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 리터럴(literal)을 사용한 리터럴 상수 값 컬럼 추가\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "df.selectExpr(\"*\", \"1 as One\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 컬럼 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# withColumn(컬럼명, 표현식) 으로 컬럼 추가\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# withColumn(컬럼명, 표현식) 으로 컬럼 추가\")\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 대소 비교를 통한 불리언 값 반환\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 대소 비교를 통한 불리언 값 반환\")\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 존재하는 컬럼을 표현식을 통해 새로운 컬럼 생성, 기존 컬럼을 삭제\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 존재하는 컬럼을 표현식을 통해 새로운 컬럼 생성, 기존 컬럼을 삭제\")\n",
    "before = df\n",
    "before.printSchema()\n",
    "\n",
    "after = before.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\"))\n",
    "after.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 컬럼명 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼 명 변경하기\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Destination', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 컬럼 명 변경하기\")\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"Destination\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 컬럼 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 특정 컬럼을 제거합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 특정 컬럼을 제거합니다\")\n",
    "df.printSchema()\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 기본적으로 스파크는 대소문자를 가리지 않지만, 옵션을 통해서 구분이 가능합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 기본적으로 스파크는 대소문자를 가리지 않지만, 옵션을 통해서 구분이 가능합니다\")\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "caseSensitive = df.drop(\"dest_country_name\")\n",
    "caseSensitive.printSchema()\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', False)\n",
    "caseInsensitive = df.drop(\"dest_country_name\")\n",
    "caseInsensitive.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 한 번에 여러 컬럼도 삭제할 수 있습니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['count']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 한 번에 여러 컬럼도 삭제할 수 있습니다\")\n",
    "df.printSchema()\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns # 여러 컬럼을 지우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 컬럼의 데이터 타입 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 데이터 유형을 변경합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|       15|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "|    United States|            Ireland|  344|      344|\n",
      "|            Egypt|      United States|   15|       15|\n",
      "|    United States|              India|   62|       62|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|int_count|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|    United States|            Romania|   15|       15|       15|\n",
      "|    United States|            Croatia|    1|        1|        1|\n",
      "|    United States|            Ireland|  344|      344|      344|\n",
      "|            Egypt|      United States|   15|       15|       15|\n",
      "|    United States|              India|   62|       62|       62|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      " |-- int_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 데이터 유형을 변경합니다\")\n",
    "df.printSchema()\n",
    "\n",
    "int2str = df.withColumn(\"str_count\", col(\"count\").cast(\"string\"))\n",
    "int2str.show(5)\n",
    "int2str.printSchema()\n",
    "\n",
    "str2int = int2str.withColumn(\"int_count\", col(\"str_count\").cast(\"int\"))\n",
    "str2int.show(5)\n",
    "str2int.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 레코드 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Where 와 Filter 는 동일합니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "# 같은 표현식에 여러 필터를 적용하는 것도 가능합니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Where 와 Filter 는 동일합니다\")\n",
    "df.where(\"count < 2\").show(2)\n",
    "df.filter(\"count < 2\").show(2)\n",
    "\n",
    "print(\"# 같은 표현식에 여러 필터를 적용하는 것도 가능합니다\")\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 유일 값 (DISTINCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "\"\"\" distinct 함수 \"\"\"\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count())\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count())\n",
    "# distinctcount?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 정렬 (SORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sort 와 orderBy 함수는 동일한 효과를 가집니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# sort 와 orderBy 함수는 동일한 효과를 가집니다\")\n",
    "df.sort(\"count\").show(2)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드로 null의 정렬 순서를 지정\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "print(\"# asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드로 null의 정렬 순서를 지정\")\n",
    "df.sort(\"DEST_COUNTRY_NAME\").show(1)\n",
    "df.sort(df[\"DEST_COUNTRY_NAME\"].asc_nulls_first()).show(1)\n",
    "df.sort(df.DEST_COUNTRY_NAME.asc_nulls_first()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 정렬의 경우 예약어 컬럼명에 유의해야 하므로, expr 을 사용하거나, 명시적으로 구조화 API 를 사용하는 것도 좋습니다\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Vietnam|    2|\n",
      "|    United States|          Venezuela|  246|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|             Angola|   13|\n",
      "|    United States|           Anguilla|   38|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 정렬의 경우 예약어 컬럼명에 유의해야 하므로, expr 을 사용하거나, 명시적으로 구조화 API 를 사용하는 것도 좋습니다\") \n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(df[\"count\"].desc()).show(2)\n",
    "df.orderBy(df.ORIGIN_COUNTRY_NAME.desc(), df.DEST_COUNTRY_NAME.asc()).show(2)\n",
    "df.orderBy(expr(\"ORIGIN_COUNTRY_NAME DESC\"), expr(\"DEST_COUNTRY_NAME ASC\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 로우 수 제한 (LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 기타 데이터 프레임 연산자\n",
    "\n",
    "### 5.1 사전에 스키마를 정의하는 장점\n",
    "* 데이터 타입을 추론에 대한 신경을 쓸 필요가 없다\n",
    "* 스키마 추론을 위한 별도의 작업에 드는 리소스를 줄일 수 있다\n",
    "* 스키마에 맞지 않는 데이터의 오류를 빠르게 인지할 수 있다\n",
    "\n",
    "### 5.2 스키마를 정의하는 두 가지 방법\n",
    "* 1. 프로그래밍 방식으로 정의하는 방법\n",
    "* 2. DDL 구문을 이용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1. Programming Style\n",
      "StructType(List(StructField(author,StringType,false),StructField(title,StringType,false),StructField(pages,IntegerType,false)))\n",
      "root\n",
      " |-- author: string (nullable = false)\n",
      " |-- title: string (nullable = false)\n",
      " |-- pages: integer (nullable = false)\n",
      "\n",
      "+------+---------------------------+-----+\n",
      "|author|title                      |pages|\n",
      "+------+---------------------------+-----+\n",
      "|박수혁|안녕하세요 psyoblade 입니다|300  |\n",
      "|황치영|안녕하세요 chiyoung 입니다 |200  |\n",
      "|유승학|안녕하세요 shryu 입니다    |100  |\n",
      "+------+---------------------------+-----+\n",
      "\n",
      "\n",
      "# 2. DDL Style\n",
      "`author` string, `title` string, `pages` int\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- pages: integer (nullable = true)\n",
      "\n",
      "+------+---------------------------+-----+\n",
      "|author|title                      |pages|\n",
      "+------+---------------------------+-----+\n",
      "|박수혁|안녕하세요 psyoblade 입니다|300  |\n",
      "|황치영|안녕하세요 chiyoung 입니다 |200  |\n",
      "|유승학|안녕하세요 shryu 입니다    |100  |\n",
      "+------+---------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    [\"박수혁\", \"안녕하세요 psyoblade 입니다\", 300],\n",
    "    [\"황치영\", \"안녕하세요 chiyoung 입니다\", 200],\n",
    "    [\"유승학\", \"안녕하세요 shryu 입니다\", 100]\n",
    "]\n",
    "\n",
    "print(\"# 1. Programming Style\")\n",
    "schema1 = StructType([\n",
    "    StructField(\"author\", StringType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"pages\", IntegerType(), False),\n",
    "])\n",
    "print(schema1)\n",
    "df1 = spark.createDataFrame(data, schema1)\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "rows = [\n",
    "    Row(\"박수혁\", \"안녕하세요 psyoblade 입니다\", 300),\n",
    "    Row(\"황치영\", \"안녕하세요 chiyoung 입니다\", 200),\n",
    "    Row(\"유승학\", \"안녕하세요 shryu 입니다\", 100)\n",
    "]\n",
    "\n",
    "print(\"\\n# 2. DDL Style\")\n",
    "schema2 = \"`author` string, `title` string, `pages` int\"\n",
    "print(schema2)\n",
    "df2 = spark.createDataFrame(rows, schema2)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "assert(df1.subtract(df2).count() == 0)\n",
    "assert(df2.subtract(df1).count() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 중첩된 배열 스키마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---+-----+-----+-----------------+---------+----+-------------------+\n",
      "|Id |First|Last |Url              |Published|Hits|Campaigns          |\n",
      "+---+-----+-----+-----------------+---------+----+-------------------+\n",
      "|1  |Jules|Damji|https://tinyurl.1|1/4/2016 |4535|[twitter, LinkedIn]|\n",
      "+---+-----+-----+-----------------+---------+----+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), False),\n",
    "    StructField(\"First\", StringType(), False),\n",
    "    StructField(\"Last\", StringType(), False),\n",
    "    StructField(\"Url\", StringType(), False),\n",
    "    StructField(\"Published\", StringType(), False),\n",
    "    StructField(\"Hits\", IntegerType(), False),\n",
    "    StructField(\"Campaigns\", ArrayType(StringType()), False),\n",
    "])\n",
    "blogDF = spark.read.schema(schema).json(\"data/learning-spark/blogs.json\")\n",
    "blogDF.printSchema()\n",
    "blogDF.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 컬럼과 표현식\n",
    "> 컬럼은 공용 메소드들을 가진 객체들이며, pyspark.sql.functions.expr() 함수를 이용하여 표현식을 그대로 사용할 수 있습니다\n",
    "\n",
    "* 특히 컬럼 함수를 통해 다양한 연산자를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']\n",
      "Help on class Column in module pyspark.sql.column:\n",
      "\n",
      "class Column(builtins.object)\n",
      " |  Column(jc)\n",
      " |  \n",
      " |  A column in a DataFrame.\n",
      " |  \n",
      " |  :class:`Column` instances can be created by::\n",
      " |  \n",
      " |      # 1. Select a column out of a DataFrame\n",
      " |  \n",
      " |      df.colName\n",
      " |      df[\"colName\"]\n",
      " |  \n",
      " |      # 2. Create from an expression\n",
      " |      df.colName + 1\n",
      " |      1 / df.colName\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __and__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __bool__ = __nonzero__(self)\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |      # container operators\n",
      " |  \n",
      " |  __div__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __eq__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ge__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, k)\n",
      " |  \n",
      " |  __gt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __init__(self, jc)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __invert__ = _(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __lt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ne__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __neg__ = _(self)\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __or__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __pow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __radd__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rand__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rdiv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rmul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ror__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rpow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __rsub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rtruediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __sub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __truediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  alias(self, *alias, **kwargs)\n",
      " |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      " |      return more than one column, such as explode).\n",
      " |      \n",
      " |      :param alias: strings of desired column names (collects all positional arguments passed)\n",
      " |      :param metadata: a dict of information to be stored in ``metadata`` attribute of the\n",
      " |          corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      " |          only argument)\n",
      " |      \n",
      " |      .. versionchanged:: 2.2\n",
      " |         Added optional ``metadata`` argument.\n",
      " |      \n",
      " |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      " |      [Row(age2=2), Row(age2=5)]\n",
      " |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      " |      99\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  asc = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_first = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      return before non-null values.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  asc_nulls_last = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  astype = cast(self, dataType)\n",
      " |      :func:`astype` is an alias for :func:`cast`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  between(self, lowerBound, upperBound)\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is between the given columns.\n",
      " |      \n",
      " |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      " |      +-----+---------------------------+\n",
      " |      | name|((age >= 2) AND (age <= 4))|\n",
      " |      +-----+---------------------------+\n",
      " |      |Alice|                       true|\n",
      " |      |  Bob|                      false|\n",
      " |      +-----+---------------------------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  bitwiseAND = _(self, other)\n",
      " |      Compute bitwise AND of this expression with another expression.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column` to calculate bitwise and(&) against\n",
      " |                    this :class:`Column`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      " |      [Row((a & b)=10)]\n",
      " |  \n",
      " |  bitwiseOR = _(self, other)\n",
      " |      Compute bitwise OR of this expression with another expression.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column` to calculate bitwise or(|) against\n",
      " |                    this :class:`Column`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      " |      [Row((a | b)=235)]\n",
      " |  \n",
      " |  bitwiseXOR = _(self, other)\n",
      " |      Compute bitwise XOR of this expression with another expression.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column` to calculate bitwise xor(^) against\n",
      " |                    this :class:`Column`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      " |      [Row((a ^ b)=225)]\n",
      " |  \n",
      " |  cast(self, dataType)\n",
      " |      Convert the column into type ``dataType``.\n",
      " |      \n",
      " |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  contains = _(self, other)\n",
      " |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      :param other: string in line\n",
      " |      \n",
      " |      >>> df.filter(df.name.contains('o')).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  desc = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_first = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear before non-null values.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  desc_nulls_last = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  endswith = _(self, other)\n",
      " |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      :param other: string at end of line (do not use a regex `$`)\n",
      " |      \n",
      " |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  eqNullSafe = _(self, other)\n",
      " |      Equality test that is safe for null values.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column`\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df1 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value='foo'),\n",
      " |      ...     Row(id=2, value=None)\n",
      " |      ... ])\n",
      " |      >>> df1.select(\n",
      " |      ...     df1['value'] == 'foo',\n",
      " |      ...     df1['value'].eqNullSafe('foo'),\n",
      " |      ...     df1['value'].eqNullSafe(None)\n",
      " |      ... ).show()\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |         true|           true|           false|\n",
      " |      |         null|          false|            true|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(value = 'bar'),\n",
      " |      ...     Row(value = None)\n",
      " |      ... ])\n",
      " |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      " |      0\n",
      " |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      " |      1\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value=float('NaN')),\n",
      " |      ...     Row(id=2, value=42.0),\n",
      " |      ...     Row(id=3, value=None)\n",
      " |      ... ])\n",
      " |      >>> df2.select(\n",
      " |      ...     df2['value'].eqNullSafe(None),\n",
      " |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      " |      ...     df2['value'].eqNullSafe(42.0)\n",
      " |      ... ).show()\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |           false|           true|           false|\n",
      " |      |           false|          false|            true|\n",
      " |      |            true|          false|           false|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      \n",
      " |      .. note:: Unlike Pandas, PySpark doesn't consider NaN values to be NULL.\n",
      " |         See the `NaN Semantics`_ for details.\n",
      " |      .. _NaN Semantics:\n",
      " |         https://spark.apache.org/docs/latest/sql-programming-guide.html#nan-semantics\n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  getField(self, name)\n",
      " |      An expression that gets a field by name in a StructField.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      " |      >>> df.select(df.r.getField(\"b\")).show()\n",
      " |      +---+\n",
      " |      |r.b|\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      >>> df.select(df.r.a).show()\n",
      " |      +---+\n",
      " |      |r.a|\n",
      " |      +---+\n",
      " |      |  1|\n",
      " |      +---+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  getItem(self, key)\n",
      " |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      " |      or gets an item by key out of a dict.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      " |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  isNotNull = _(self)\n",
      " |      True if the current expression is NOT null.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNotNull()).collect()\n",
      " |      [Row(height=80, name='Tom')]\n",
      " |  \n",
      " |  isNull = _(self)\n",
      " |      True if the current expression is null.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNull()).collect()\n",
      " |      [Row(height=None, name='Alice')]\n",
      " |  \n",
      " |  isin(self, *cols)\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is contained by the evaluated values of the arguments.\n",
      " |      \n",
      " |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  like = _(self, other)\n",
      " |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      " |      \n",
      " |      :param other: a SQL LIKE pattern\n",
      " |      \n",
      " |      See :func:`rlike` for a regex version\n",
      " |      \n",
      " |      >>> df.filter(df.name.like('Al%')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  name = alias(self, *alias, **kwargs)\n",
      " |      :func:`name` is an alias for :func:`alias`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  otherwise(self, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      See :func:`pyspark.sql.functions.when` for example usage.\n",
      " |      \n",
      " |      :param value: a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      " |      +-----+-------------------------------------+\n",
      " |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      " |      +-----+-------------------------------------+\n",
      " |      |Alice|                                    0|\n",
      " |      |  Bob|                                    1|\n",
      " |      +-----+-------------------------------------+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  over(self, window)\n",
      " |      Define a windowing column.\n",
      " |      \n",
      " |      :param window: a :class:`WindowSpec`\n",
      " |      :return: a Column\n",
      " |      \n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> window = Window.partitionBy(\"name\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      " |      >>> from pyspark.sql.functions import rank, min\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.withColumn(\"rank\", rank().over(window))                 .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()\n",
      " |      +---+-----+----+---+\n",
      " |      |age| name|rank|min|\n",
      " |      +---+-----+----+---+\n",
      " |      |  5|  Bob|   1|  5|\n",
      " |      |  2|Alice|   1|  2|\n",
      " |      +---+-----+----+---+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rlike = _(self, other)\n",
      " |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      " |      match.\n",
      " |      \n",
      " |      :param other: an extended regex expression\n",
      " |      \n",
      " |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  startswith = _(self, other)\n",
      " |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      :param other: string at start of line (do not use a regex `^`)\n",
      " |      \n",
      " |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  substr(self, startPos, length)\n",
      " |      Return a :class:`Column` which is a substring of the column.\n",
      " |      \n",
      " |      :param startPos: start position (int or Column)\n",
      " |      :param length:  length of the substring (int or Column)\n",
      " |      \n",
      " |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      " |      [Row(col='Ali'), Row(col='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  when(self, condition, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      See :func:`pyspark.sql.functions.when` for example usage.\n",
      " |      \n",
      " |      :param condition: a boolean :class:`Column` expression.\n",
      " |      :param value: a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      |Alice|                                                          -1|\n",
      " |      |  Bob|                                                           1|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import Column\n",
    "print(blogDF.columns)\n",
    "help(Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      AuthorsId|\n",
      "+---------------+\n",
      "|  Jules.Damji@1|\n",
      "| Brooke.Wenig@2|\n",
      "|    Denny.Lee@3|\n",
      "|Tathagata.Das@4|\n",
      "+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.withColumn(\"AuthorsId\", (concat(expr(\"First\"), lit(\".\"), expr(\"Last\"), lit(\"@\"), expr(\"Id\"))))\\\n",
    ".select(col(\"AuthorsId\"))\\\n",
    ".show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.select(expr(\"Hits\")).show(2)\n",
    "blogDF.select(col(\"Hits\")).show(2)\n",
    "blogDF.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.sort(col(\"Id\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 로우 생성 및 다루기\n",
    "* 로우의 경우 컬럼을 인덱스를 기준으로 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reynold\n",
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    "[\"twitter\", \"LinkedIn\"])\n",
    "print(blog_row[1])\n",
    "\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+-------------+\n",
      "|CallNumber|UnitID|IncidentNumber|CallType        |CallDate  |RowID        |\n",
      "+----------+------+--------------+----------------+----------+-------------+\n",
      "|20110016  |T13   |2003235       |Structure Fire  |01/11/2002|020110016-T13|\n",
      "|20110022  |M17   |2003241       |Medical Incident|01/11/2002|020110022-M17|\n",
      "|20110023  |M41   |2003242       |Medical Incident|01/11/2002|020110023-M41|\n",
      "|20110032  |E11   |2003250       |Vehicle Fire    |01/11/2002|020110032-E11|\n",
      "|20110043  |B04   |2003259       |Alarms          |01/11/2002|020110043-B04|\n",
      "|20110072  |T08   |2003279       |Structure Fire  |01/11/2002|020110072-T08|\n",
      "|20110125  |E33   |2003301       |Alarms          |01/11/2002|020110125-E33|\n",
      "|20110130  |E36   |2003304       |Alarms          |01/11/2002|020110130-E36|\n",
      "|20110197  |E05   |2003343       |Medical Incident|01/11/2002|020110197-E05|\n",
      "|20110215  |E06   |2003348       |Medical Incident|01/11/2002|020110215-E06|\n",
      "+----------+------+--------------+----------------+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python, define a schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "StructField('UnitID', StringType(), True),\n",
    "StructField('IncidentNumber', IntegerType(), True),\n",
    "StructField('CallType', StringType(), True),\n",
    "StructField('CallDate', StringType(), True),\n",
    "StructField('WatchDate', StringType(), True),\n",
    "StructField('CallFinalDisposition', StringType(), True),\n",
    "StructField('AvailableDtTm', StringType(), True),\n",
    "StructField('Address', StringType(), True),\n",
    "StructField('City', StringType(), True),\n",
    "StructField('Zipcode', IntegerType(), True),\n",
    "StructField('Battalion', StringType(), True),\n",
    "StructField('StationArea', StringType(), True),\n",
    "StructField('Box', StringType(), True),\n",
    "StructField('OriginalPriority', StringType(), True),\n",
    "StructField('Priority', StringType(), True),\n",
    "StructField('FinalPriority', IntegerType(), True),\n",
    "StructField('ALSUnit', BooleanType(), True),\n",
    "StructField('CallTypeGroup', StringType(), True),\n",
    "StructField('NumAlarms', IntegerType(), True),\n",
    "StructField('UnitType', StringType(), True),\n",
    "StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "StructField('FirePreventionDistrict', StringType(), True),\n",
    "StructField('SupervisorDistrict', StringType(), True),\n",
    "StructField('Neighborhood', StringType(), True),\n",
    "StructField('Location', StringType(), True),\n",
    "StructField('RowID', StringType(), True),\n",
    "StructField('Delay', FloatType(), True)])\n",
    "\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"data/learning-spark/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n",
    "fire_df.select(\"CallNumber\", \"UnitID\", \"IncidentNumber\", \"CallType\", \"CallDate\", \"RowID\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 파케이 파일 혹은 테이블 저장\n",
    "* save 저장 시에는 해당 경로에 파케이 파일이 저장되고, saveAsTable 저장 시에는 \"spark.sql.warehouse.dir\" 의 위치에 생성됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetPath=\"target/sf_fire_calls\"\n",
    "fire_df.write.format(\"parquet\").save(parquetPath)\n",
    "\n",
    "parquetTable=\"sf_fire_calls\"\n",
    "fire_df.write.format(\"parquet\").saveAsTable(parquetTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 프로젝션과 필터\n",
    "> *Projection*은 특정 관계형 조건 혹은 필터에 매칭되는 로우에 대해서만 반환하는 것을 말합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[IncidentNumber: int, AvailableDtTm: string, CallType: string]\n",
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_fire_df = (fire_df\n",
    ".select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    ".where(col(\"CallType\") != \"Medical Incident\"))\n",
    "print(few_fire_df)\n",
    "few_fire_df.show(5, truncate=False)\n",
    "\n",
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(new_fire_df\n",
    ".select(\"ResponseDelayedinMins\")\n",
    ".where(col(\"ResponseDelayedinMins\") > 5)\n",
    ".show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 날짜 관련 함수\n",
    "* 날짜의 경우 문자열로 전달되고 있기 때문에 표현 및 활용을 위해서는 to_timestamp(), to_date() 와 같은 날짜관련 함수를 사용할 수 있습니다.\n",
    "  - 한번 timestamp 형태로 변경된 컬럼에 대해서는 year, month, dayofmonth 와 같은 일자관련 함수를 통해 다양한 예제를 실습할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------+-------------------+------------------------+\n",
      "|year(IncidentDate)|month(IncidentDate)|dayofmonth(IncidentDate)|\n",
      "+------------------+-------------------+------------------------+\n",
      "|              2000|                  4|                      12|\n",
      "|              2000|                  4|                      13|\n",
      "|              2000|                  4|                      14|\n",
      "|              2000|                  4|                      15|\n",
      "|              2000|                  4|                      16|\n",
      "+------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df = (new_fire_df\n",
    ".withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"CallDate\")\n",
    ".withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"WatchDate\")\n",
    ".withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    "\"MM/dd/yyyy hh:mm:ss a\"))\n",
    ".drop(\"AvailableDtTm\"))\n",
    "fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").show(5, truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "(fire_ts_df\n",
    ".select(year('IncidentDate'), month('IncidentDate'), dayofmonth(\"IncidentDate\"))\n",
    ".distinct()\n",
    ".orderBy(year('IncidentDate'), month('IncidentDate'), dayofmonth(\"IncidentDate\"))\n",
    ".show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터셋 API\n",
    "> Python 과 R 은 compile-time type-safe 하지 않기 때문에, Datasets 통한 Typed 데이터 타입을 사용할 수 없습니다. Datasets 을 이용하는 경우에도 Spark SQL 엔진이 객체를 생성, 변환, 직렬화, 역직렬화를 수행하며, **Dataframe 의 경우와 마찬가지로 Off-heap 을 통한 메모리 관리를 수행**하게 되며, Dataset encoders 를 이용합니다\n",
    "\n",
    "### 6.1 데이터셋과 데이터프레임 비교\n",
    "\n",
    "| Structured APIs | SQL vs. Dataframe vs. Datasets |\n",
    "|---|---|\n",
    "| ![structured-api](images/structured-api.png) | ![sql-vs-dataframes-vs-datasets-type-safety-spectrum](images/sql-vs-dataframes-vs-datasets-type-safety-spectrum.png) |\n",
    "\n",
    "* 언어별 타입 객체 비교\n",
    "![typed-untyped](images/typed-untyped.png)\n",
    "\n",
    "* Scala: Case Class 를 통해 선언\n",
    "```scala\n",
    "case class DeviceIoTData (\n",
    "    battery_level: Long, \n",
    "    c02_level: Long,\n",
    "    cca2: String, \n",
    "    cca3: String, \n",
    "    cn: String, \n",
    "    device_id: Long,\n",
    "    device_name: String, \n",
    "    humidity: Long, \n",
    "    ip: String, \n",
    "    latitude: Double,\n",
    "    lcd: String, \n",
    "    longitude: Double, \n",
    "    scale:String, \n",
    "    temp: Long,\n",
    "    timestamp: Long)\n",
    "```\n",
    "\n",
    "* 데이터를 읽고 DeviceIoTData 클래스로 변환을 수행합니다\n",
    "```scala\n",
    "val ds = spark.read.json(\"/databricks-datasets/learning-spark-v2/iot-devices/iot_devices.json\").as[DeviceIoTData]\n",
    "val filterTempDS = ds.filter({d => {d.temp > 30 && d.humidity > 70})\n",
    "```\n",
    "* Datasets 이용 시에는 filter(), map(), groupBy(), select(), take() 등의 일반적인 함수를 사용합니다\n",
    "```scala                              \n",
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long, cca3: String)\n",
    "val dsTemp = ds.filter(d => {d.temp > 25})\n",
    "    .map(d => (d.temp, d.device_name, d.device_id, d.cca3))\n",
    "    .toDF(\"temp\", \"device_name\", \"device_id\", \"cca3\")\n",
    "    .as[DeviceTempByCountry]\n",
    "```\n",
    "\n",
    "### 6.2 데이터셋 데이터프레임 그리고 RDD\n",
    "* Datasets\n",
    "  - compile-time 의 type safety 가 필요한 경우\n",
    "* Dataframe\n",
    "  - SQL-like 쿼리를 이용하고자 하는 경우\n",
    "  - 통합, 코드 최적화 그리고 API를 활용한 모듈화를 원하는 경우\n",
    "  - R 혹은 Python 을 이용해야 하는 경우\n",
    "  - 공간, 속도 효율성을 고려해야 하는 경우\n",
    "* RDD\n",
    "  - 별도의 RDD를 이용하는 써드파티 패키지를 사용하는 경우\n",
    "  - 코드, 공간, 속도 최적화 등을 원하지 않는 경우\n",
    "  - 스파크가 수행할 쿼리를 정확히 지시해야만 할 때\n",
    "\n",
    "\n",
    "* RDD와 데이터프레임과 데이터셋은 서로 다른가?\n",
    "  - 데이터프레임과 데이터셋은 RDD 위에서 구현됩니다. 즉, whole-stage code generation 단계에서 압축된 RDD 코드로 분해됩니다.\n",
    "\n",
    "> DataFrames and Datasets are\n",
    "built on top of RDDs, and they get decomposed to compact RDD code during wholestage\n",
    "code generation, which we discuss in the next section\n",
    "\n",
    "* Spark SQL\n",
    "![spark-sql](images/spark-sql.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 카탈리스트 옵티마이저\n",
    "> Spark SQL 엔진의 핵심이며 크게 4가지 단계로 구분됩니다. \n",
    "\n",
    "### 7.1 분석 (Analysis)\n",
    "* \"추상화 구문 트리(AST, Abstract Syntax Tree)\" 생성 단계로, 모든 테이블명과 컬럼명은 내부적으로 컬럼명, 데이터유형, 함수와 더불어 데이터베이스와 테이블 이름까지 모두 관리하고 있는 *Catalog*에 의해 해석되어 트리 형태의 구조로 생성됩니다\n",
    "\n",
    "### 7.2 논리 최적화 (Logical Optimization)\n",
    "* 카탈리스트 옵티마이저는 우선 다수의 논리적 계획을 세우고, \"비용 기반 옵티마이저(CBO, Cost-Based Optimizer)\"를 이용하여 각 계획에 비용(Cost)를 할당합니다. 이러한 계획은 아래의 \"Figure 3-5\"와 같은 연산자 트리 형태로 구성되며, 이때에 **constant folding, predicate pushdown, projection pruning, Boolean expression simplification** 등의 최적화가 이루어집니다\n",
    "\n",
    "### 7.3 물리 계획 (Physical Planning)\n",
    "* Spark SQL 엔진은 CBO에 의해 선택된 논리 계획에 대해 스파크 엔진에 존재하는 적절한 연산자들을 이용하여 최적의 계획을 생성합니다\n",
    "\n",
    "### 7.4 코드 생성 (Code Generation)\n",
    "* 마지막 단계에서는  Project Tungsten 의 whole-stage code generation 을 통해 [마치 컴파일러와 같이 동작](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)하며, 메모리 상에 로딩된 데이터 집합에 대해 수행될 최적의 자바 바이트 코드를 생성해 냅니다.\n",
    "\n",
    "* What is ***whole-stage code generation***?\n",
    "  - 물리적인 쿼리 최적화 단계를 말하며, 쿼리 전체를 하나의 함수로 만들어 냅니다\n",
    "  - virtual function call 을 제거하거나, 중간 데이터를 CPU registers 에 올리는 등의 최적화 작업을 수행합니다\n",
    "  - Spark 2.0 텅스텐 엔진은 압축된 RDD 코드를 생성하는 방식으로 개선 되었습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Total DESC NULLS LAST], true\n",
      "+- Aggregate [State#5225, Color#5226, Count#5227], [State#5225, Color#5226, Count#5227, sum(cast(Count#5227 as bigint)) AS Total#5238L]\n",
      "   +- Project [State#5225, Color#5226, Count#5227]\n",
      "      +- Relation[State#5225,Color#5226,Count#5227] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "State: string, Color: string, Count: int, Total: bigint\n",
      "Sort [Total#5238L DESC NULLS LAST], true\n",
      "+- Aggregate [State#5225, Color#5226, Count#5227], [State#5225, Color#5226, Count#5227, sum(cast(Count#5227 as bigint)) AS Total#5238L]\n",
      "   +- Project [State#5225, Color#5226, Count#5227]\n",
      "      +- Relation[State#5225,Color#5226,Count#5227] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Total#5238L DESC NULLS LAST], true\n",
      "+- Aggregate [State#5225, Color#5226, Count#5227], [State#5225, Color#5226, Count#5227, sum(cast(Count#5227 as bigint)) AS Total#5238L]\n",
      "   +- Relation[State#5225,Color#5226,Count#5227] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [Total#5238L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(Total#5238L DESC NULLS LAST, 200), true, [id=#2271]\n",
      "   +- *(2) HashAggregate(keys=[State#5225, Color#5226, Count#5227], functions=[sum(cast(Count#5227 as bigint))], output=[State#5225, Color#5226, Count#5227, Total#5238L])\n",
      "      +- Exchange hashpartitioning(State#5225, Color#5226, Count#5227, 200), true, [id=#2267]\n",
      "         +- *(1) HashAggregate(keys=[State#5225, Color#5226, Count#5227], functions=[partial_sum(cast(Count#5227 as bigint))], output=[State#5225, Color#5226, Count#5227, sum#5244L])\n",
      "            +- FileScan csv [State#5225,Color#5226,Count#5227] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/databricks/mnm_dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Color:string,Count:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/databricks/mnm_dataset.csv\")\n",
    ")\n",
    "count_mnm_df = (\n",
    "    mnm_df.select(\"State\", \"Color\", \"Count\")\n",
    "    .groupBy(\"State\", \"Color\", \"Count\")\n",
    "    .agg(sum(\"Count\")\n",
    "    .alias(\"Total\"))\n",
    "    .orderBy(\"Total\", ascending=False)\n",
    ")\n",
    "count_mnm_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Total DESC NULLS LAST], true\n",
      "+- 'Aggregate ['State, 'Color, 'Count], ['State, 'Color, 'Count, 'sum('Count) AS Total#5167]\n",
      "   +- 'UnresolvedRelation [mnm_dataset]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "State: string, Color: string, Count: int, Total: bigint\n",
      "Sort [Total#5167L DESC NULLS LAST], true\n",
      "+- Aggregate [State#5147, Color#5148, Count#5149], [State#5147, Color#5148, Count#5149, sum(cast(Count#5149 as bigint)) AS Total#5167L]\n",
      "   +- SubqueryAlias mnm_dataset\n",
      "      +- Relation[State#5147,Color#5148,Count#5149] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Total#5167L DESC NULLS LAST], true\n",
      "+- Aggregate [State#5147, Color#5148, Count#5149], [State#5147, Color#5148, Count#5149, sum(cast(Count#5149 as bigint)) AS Total#5167L]\n",
      "   +- Relation[State#5147,Color#5148,Count#5149] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [Total#5167L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(Total#5167L DESC NULLS LAST, 200), true, [id=#2178]\n",
      "   +- *(2) HashAggregate(keys=[State#5147, Color#5148, Count#5149], functions=[sum(cast(Count#5149 as bigint))], output=[State#5147, Color#5148, Count#5149, Total#5167L])\n",
      "      +- Exchange hashpartitioning(State#5147, Color#5148, Count#5149, 200), true, [id=#2174]\n",
      "         +- *(1) HashAggregate(keys=[State#5147, Color#5148, Count#5149], functions=[partial_sum(cast(Count#5149 as bigint))], output=[State#5147, Color#5148, Count#5149, sum#5175L])\n",
      "            +- FileScan csv [State#5147,Color#5148,Count#5149] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/databricks/mnm_dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Color:string,Count:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df.createOrReplaceTempView(\"mnm_dataset\")\n",
    "count_mnm_df = spark.sql(\"SELECT State, Color, Count, sum(Count) AS Total \\\n",
    "FROM mnm_dataset \\\n",
    "GROUP BY State, Color, Count \\\n",
    "ORDER BY Total DESC\")\n",
    "count_mnm_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아래와 같이 2개의 테이블에 대해 조인, 필터, 프로젝션 등의 연산 시에 아래와 같은 최적화로 **Disk 및 Network I/O 를 줄일 수 있습니다**.\n",
    "  - Predicate Pushdown : 데이터 소스를 모두 읽지 않고, 필터 조건에 해당하는 데이터만 읽습니다\n",
    "  - Column Pruning : 데이터 소스에서 모든 필드를 읽지 않고, 필요한 필터만 읽습니다\n",
    "\n",
    "```scala\n",
    "val users = spark.read.parquet(\"/users/parquet/path\")\n",
    "val events = spark.read.parquet(\"/events/parquet/path\")\n",
    "val joinedDF = users.join(events, users(\"id\") === events(\"uid\"))\n",
    ".filter(events(\"date\") > \"2015-01-01\")\n",
    "```\n",
    "\n",
    "![query-transformation](images/query-transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 실습 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습#1 가장 최근에 가입(u_signup)한 5명을 출력하세요\n",
    "* 이용자 데이터 CSV \"data/tbl_user.csv\" 파일을 읽고, \n",
    "> 참고: sort, desc, from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+--------+\n",
      "|u_id|    u_name|u_gender|u_signup|\n",
      "+----+----------+--------+--------+\n",
      "|   9|  최컴퓨터|      남|20201124|\n",
      "|   8|  조노트북|      여|20161201|\n",
      "|   7|  임모바일|      남|20040807|\n",
      "|   6|  윤디오스|      남|20040101|\n",
      "|   5|유코드제로|      여|20021029|\n",
      "+----+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "user = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/tbl_user.csv\")\n",
    "user.createOrReplaceTempView(\"user\")\n",
    "spark.sql(\"select * from user order by u_signup desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습#2 구매 금액이 200만원 (p_amount) 이상 구매한 이용자 목록을 출력하세요\n",
    "* 구매 데이터 CSV \"data/tbl_purchase.csv\" 파일을 읽어서 해결\n",
    "> 참고: filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----------+--------+\n",
      "|    p_time|p_uid|p_id|     p_name|p_amount|\n",
      "+----------+-----+----+-----------+--------+\n",
      "|1603674500|    4|2003|LG Computer| 4500000|\n",
      "|1603665955|    5|2004|    LG Gram| 3500000|\n",
      "|1603666155|    5|2004|      LG TV| 2500000|\n",
      "+----------+-----+----+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchase = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/tbl_purchase.csv\")\n",
    "purchase.filter(\"p_amount > 2000000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습#3 가장 많이 발생하는 호출유형(CallType)은 무엇인가?\n",
    "* 샌프란시스코 화재발생 데이터 CSV 파일은 \"data/learning-spark/sf-fire-calls.csv\" 입니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: double (nullable = true)\n",
      "\n",
      "+----------------+\n",
      "|CallType        |\n",
      "+----------------+\n",
      "|Structure Fire  |\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|Vehicle Fire    |\n",
      "|Alarms          |\n",
      "|Structure Fire  |\n",
      "|Alarms          |\n",
      "|Alarms          |\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firecalls = (\n",
    "    spark.read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"data/learning-spark/sf-fire-calls.csv\")\n",
    ")\n",
    "firecalls.printSchema()\n",
    "firecalls.select(\"CallType\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------+\n",
      "|CallType                     |CallCount|\n",
      "+-----------------------------+---------+\n",
      "|Medical Incident             |113794   |\n",
      "|Structure Fire               |23319    |\n",
      "|Alarms                       |19406    |\n",
      "|Traffic Collision            |7013     |\n",
      "|Citizen Assist / Service Call|2524     |\n",
      "+-----------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    firecalls\n",
    "    .where(expr(\"CallType is not null\"))  # .col(\"CallType\").isNotNull()\n",
    "    .select(\"CallType\")\n",
    "    .groupBy(\"CallType\")\n",
    "    .agg(count(\"CallType\").alias(\"CallCount\"))  # .count()\n",
    "    .orderBy(desc(\"CallCount\")) # .orderBy(\"CallCount\", ascending=False), .orderBy(\"count('CallType')\") w/o alias\n",
    ")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습#4 그 이외의 질문\n",
    "* 2018 년의 모든 화재 신고 유형은 무엇 이었습니까?\n",
    "* 2018 년의 몇 월에 화재 신고가 가장 많았습니까?\n",
    "* 샌프란시스코에서 2018 년에 가장 많은 화재 신고가 발생한 지역은 어디입니까?\n",
    "* 2018 년에 화재 신고에 대한 응답 시간이 가장 나쁜 지역은 어디입니까?\n",
    "* 2018 년 중 어느 주에 화재 신고가 가장 많았습니까?\n",
    "* 이웃, 우편 번호, 화재 전화 건수간에 상관 관계가 있습니까?\n",
    "* Parquet 파일 또는 SQL 테이블을 사용하여이 데이터를 저장하고 다시 읽을 수있는 방법은 무엇입니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
