{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3일차 2교시 기본 연산 다루기\n",
    "\n",
    "> 스파크의 \"기본 연산자\" 와 \"데이터프레임\"에 대해 학습합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기본 연산자\n",
    "---\n",
    "### 1-1. 데이터 프레임 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.printSchema() | 스키마 정보를 출력합니다. | - |\n",
    "| df.schema | StructType 스키마를 반환합니다 | - |\n",
    "| df.columns | 컬럼명 정보를 반환합니다 | - |\n",
    "| df.show(n) | 데이터 n 개를 출력합니다 | - |\n",
    "| df.first() | 데이터 프레임의 첫 번째 Row 를 반환합니다 | - |\n",
    "| df.head(n) | 데이터 프레임의 처음부터 n 개의 Row 를 반환합니다 | - |\n",
    "| df.createOrReplaceTempView | 임시 뷰 테이블을 생성합니다 | - |\n",
    "| df.union(newdf) | 데이터프레임 간의 유니온 연산을 수행합니다 | - |\n",
    "| df.limit(n) | 추출할 로우수 제한 | T |\n",
    "| df.repartition(n) | 파티션 재분배, 셔플발생 | - |\n",
    "| df.coalesce() | 셔플하지 않고 파티션을 병합 | 마지막 스테이지의 reduce 수가 줄어드는 효과로 성능저하에 유의해야 합니다 |\n",
    "| df.collect() | 모든 데이터 수집, 반환 | A |\n",
    "| df.take(n) | 상위 n개 로우 반환 | A |\n",
    "\n",
    "---\n",
    "### 1-2. 컬럼 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.select | 컬럼이나 표현식 사용  | - |\n",
    "| df.selectExpr | 문자열 표현식 사용 = df.select(expr()) | - |\n",
    "| df.withColumn(컬럼명, 표현식) | 컬럼 추가, 비교, 컬럼명 변경 | - |\n",
    "| df.withColumnRenamed(old_name, new_name) | 컬럼명 변경 | - |\n",
    "| df.drop() | 컬럼 삭제 | - |\n",
    "| df.where | 로우 필터링 | - |\n",
    "| df.filter | 로우 필터링 | - |\n",
    "| df.sort, df.orderBy | 정렬 | - |\n",
    "| df.sortWithinPartitions | 파티션별 정렬 | - |\n",
    "\n",
    "---\n",
    "### 1-3. 기타 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| expr(\"someCol - 5\") | 표현식 | - |\n",
    "| lit() | 리터럴 | - |\n",
    "| cast() | 컬럼 데이터 타입 변경 | - |\n",
    "| distinct() | unique row | - |\n",
    "| desc(), asc() | 정렬 순서 | - |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 스파크 데이터 다루기\n",
    "\n",
    "### 2.1 파일로 부터 테이블 만들어 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 원시 데이터로 부터 읽거나, Spark SQL 통한 결과는 항상 데이터프레임이 생성됩니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 원시 데이터로 부터 읽거나, Spark SQL 통한 결과는 항상 데이터프레임이 생성됩니다\")\n",
    "df = spark.read.json(\"data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"2015_summary\")\n",
    "\n",
    "sql_result = spark.sql(\"SELECT * FROM 2015_summary\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 특정 컬럼 선택 (select, selectExpr)\n",
    "> 아래의 모든 예제에서 컬럼 선택 시에 select(col(\"컬럼명\")) 으로 접근할 수도 있지만 **selectExpr(\"컬럼명\") 이 간결하기 때문에 앞으로는 가능한 표현식으로 사용**하겠습니다 <br>\n",
    "컬럼 표현식의 경우 반드시 하나의 컬럼은 하나씩 표현되어야만 합니다.  <br>\n",
    "잘된예 : \"컬럼1\", \"컬럼2\" <br>\n",
    "잘못된예: \"컬럼1, 컬럼2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# select 표현은 컬럼만 입력이 가능하며, 함수나 기타 표현식을 사용할 수 없습니다. 사용하기 위해서는 functions 를 임포트 하고, 개별 함수의 특징을 잘 이해하고 사용해야 합니다\n",
      "+------------------------+-------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|ORIGIN_COUNTRY_NAME|\n",
      "+------------------------+-------------------+\n",
      "|           UNITED STATES|            Romania|\n",
      "|           UNITED STATES|            Croatia|\n",
      "+------------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "# selectExpr 별도의 임포트 없이, 모든 표현식을 사용할 수 있습니다\n",
      "+------------------------+-------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|ORIGIN_COUNTRY_NAME|\n",
      "+------------------------+-------------------+\n",
      "|           UNITED STATES|            Romania|\n",
      "|           UNITED STATES|            Croatia|\n",
      "+------------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"# select 표현은 컬럼만 입력이 가능하며, 함수나 기타 표현식을 사용할 수 없습니다. 사용하기 위해서는 functions 를 임포트 하고, 개별 함수의 특징을 잘 이해하고 사용해야 합니다\")\n",
    "df.select(upper(col(\"DEST_COUNTRY_NAME\")), \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "print(\"# selectExpr 별도의 임포트 없이, 모든 표현식을 사용할 수 있습니다\")\n",
    "df.selectExpr(\"upper(DEST_COUNTRY_NAME)\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 앨리어스 혹은 전체 컬럼을 위한 * 도 사용할 수 있습니다\n",
      "+-------------+-----------------+\n",
      "| newColmnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 앨리어스 혹은 전체 컬럼을 위한 * 도 사용할 수 있습니다\")\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColmnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 상수값 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 리터럴(literal)을 사용한 리터럴 상수 값 컬럼 추가\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "df.selectExpr(\"*\", \"1 as One\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 컬럼 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# withColumn(컬럼명, 표현식) 으로 컬럼 추가\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# withColumn(컬럼명, 표현식) 으로 컬럼 추가\")\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 대소 비교를 통한 불리언 값 반환\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 대소 비교를 통한 불리언 값 반환\")\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 존재하는 컬럼을 표현식을 통해 새로운 컬럼 생성, 기존 컬럼을 삭제\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 존재하는 컬럼을 표현식을 통해 새로운 컬럼 생성, 기존 컬럼을 삭제\")\n",
    "before = df\n",
    "before.printSchema()\n",
    "\n",
    "after = before.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\"))\n",
    "after.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 컬럼명 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼 명 변경하기\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Destination', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 컬럼 명 변경하기\")\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"Destination\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 컬럼 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 특정 컬럼을 제거합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 특정 컬럼을 제거합니다\")\n",
    "df.printSchema()\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 기본적으로 스파크는 대소문자를 가리지 않지만, 옵션을 통해서 구분이 가능합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 기본적으로 스파크는 대소문자를 가리지 않지만, 옵션을 통해서 구분이 가능합니다\")\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "caseSensitive = df.drop(\"dest_country_name\")\n",
    "caseSensitive.printSchema()\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', False)\n",
    "caseInsensitive = df.drop(\"dest_country_name\")\n",
    "caseInsensitive.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 한 번에 여러 컬럼도 삭제할 수 있습니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['count']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 한 번에 여러 컬럼도 삭제할 수 있습니다\")\n",
    "df.printSchema()\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns # 여러 컬럼을 지우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 컬럼의 데이터 타입 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 데이터 유형을 변경합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|       15|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "|    United States|            Ireland|  344|      344|\n",
      "|            Egypt|      United States|   15|       15|\n",
      "|    United States|              India|   62|       62|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|int_count|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|    United States|            Romania|   15|       15|       15|\n",
      "|    United States|            Croatia|    1|        1|        1|\n",
      "|    United States|            Ireland|  344|      344|      344|\n",
      "|            Egypt|      United States|   15|       15|       15|\n",
      "|    United States|              India|   62|       62|       62|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      " |-- int_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 데이터 유형을 변경합니다\")\n",
    "df.printSchema()\n",
    "\n",
    "int2str = df.withColumn(\"str_count\", col(\"count\").cast(\"string\"))\n",
    "int2str.show(5)\n",
    "int2str.printSchema()\n",
    "\n",
    "str2int = int2str.withColumn(\"int_count\", col(\"str_count\").cast(\"int\"))\n",
    "str2int.show(5)\n",
    "str2int.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 레코드 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Where 와 Filter 는 동일합니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "# 같은 표현식에 여러 필터를 적용하는 것도 가능합니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Where 와 Filter 는 동일합니다\")\n",
    "df.where(\"count < 2\").show(2)\n",
    "df.filter(\"count < 2\").show(2)\n",
    "\n",
    "print(\"# 같은 표현식에 여러 필터를 적용하는 것도 가능합니다\")\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 유일 값 (DISTINCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "\"\"\" distinct 함수 \"\"\"\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count())\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count())\n",
    "# distinctcount?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sort 와 orderBy 함수는 동일한 효과를 가집니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# sort 와 orderBy 함수는 동일한 효과를 가집니다\")\n",
    "df.sort(\"count\").show(2)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드로 null의 정렬 순서를 지정\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "print(\"# asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드로 null의 정렬 순서를 지정\")\n",
    "df.sort(\"DEST_COUNTRY_NAME\").show(1)\n",
    "df.sort(df[\"DEST_COUNTRY_NAME\"].asc_nulls_first()).show(1)\n",
    "df.sort(df.DEST_COUNTRY_NAME.asc_nulls_first()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 정렬의 경우 예약어 컬럼명에 유의해야 하므로, expr 을 사용하거나, 명시적으로 구조화 API 를 사용하는 것도 좋습니다\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Vietnam|    2|\n",
      "|    United States|          Venezuela|  246|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|             Angola|   13|\n",
      "|    United States|           Anguilla|   38|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 정렬의 경우 예약어 컬럼명에 유의해야 하므로, expr 을 사용하거나, 명시적으로 구조화 API 를 사용하는 것도 좋습니다\") \n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(df[\"count\"].desc()).show(2)\n",
    "df.orderBy(df.ORIGIN_COUNTRY_NAME.desc(), df.DEST_COUNTRY_NAME.asc()).show(2)\n",
    "df.orderBy(expr(\"ORIGIN_COUNTRY_NAME DESC\"), expr(\"DEST_COUNTRY_NAME ASC\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 로우 수 제한 (LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습#3 data/tbl_user.csv 파일을 읽고, 가장 최근에 가입(u_signup)한 5명을 출력하세요\n",
    "> 참고: sort, desc, from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+--------+\n",
      "|u_id|    u_name|u_gender|u_signup|\n",
      "+----+----------+--------+--------+\n",
      "|   9|  최컴퓨터|      남|20201124|\n",
      "|   8|  조노트북|      여|20161201|\n",
      "|   7|  임모바일|      남|20040807|\n",
      "|   6|  윤디오스|      남|20040101|\n",
      "|   5|유코드제로|      여|20021029|\n",
      "+----+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "user = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/tbl_user.csv\")\n",
    "user.createOrReplaceTempView(\"user\")\n",
    "spark.sql(\"select * from user order by u_signup desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습#4 data/tbl_purchase.csv 파일을 읽고, 200만원 (p_amount) 이상 구매한 이용자 목록을 출력하세요\n",
    "> 참고: filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----------+--------+\n",
      "|    p_time|p_uid|p_id|     p_name|p_amount|\n",
      "+----------+-----+----+-----------+--------+\n",
      "|1603674500|    4|2003|LG Computer| 4500000|\n",
      "|1603665955|    5|2004|    LG Gram| 3500000|\n",
      "|1603666155|    5|2004|      LG TV| 2500000|\n",
      "+----------+-----+----+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchase = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/tbl_purchase.csv\")\n",
    "purchase.filter(\"p_amount > 2000000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
