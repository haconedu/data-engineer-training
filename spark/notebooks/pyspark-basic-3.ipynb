{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3일차 3교시 데이터 타입\n",
    "> 스파크에서 사용되는 데이터 타입에 대해 실습합니다\n",
    "\n",
    "### 참고 사이트\n",
    "* [PySpark Search](https://spark.apache.org/docs/latest/api/python/search.html)\n",
    "* [Pyspark Functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#module-pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" DataFrame 생성 \"\"\"\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"retail\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 리터럴 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 불리언 형 데이터 타입 다루기\n",
    "### 2.1 AND 조건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|InvoiceNO|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|InvoiceNO|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "x1 = df.where(col(\"InvoiceNO\") != 536365).select(\"InvoiceNO\", \"Description\")\n",
    "x2 = df.where(\"InvoiceNO <> 536365\").select(\"InvoiceNO\", \"Description\")\n",
    "x3 = df.where(\"InvoiceNO = 536365\").select(\"InvoiceNO\", \"Description\")\n",
    "\n",
    "x1.show(2)\n",
    "x2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 OR 조건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536370|     POST|       POSTAGE|       3|2010-12-01 08:45:00|     18.0|   12583.0|        France|\n",
      "|   536403|     POST|       POSTAGE|       1|2010-12-01 11:27:00|     15.0|   12791.0|   Netherlands|\n",
      "|   536527|     POST|       POSTAGE|       1|2010-12-01 13:04:00|     18.0|   12662.0|       Germany|\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "df.where(\"UnitPrice > 600 OR instr(Description, 'POSTAGE') >= 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ISIN - 제공된 목록에 포함되었는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|StockCode|\n",
      "+---------+\n",
      "|      DOT|\n",
      "|     POST|\n",
      "|       C2|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL 을 이용한 is in 구문 사용\n",
    "from pyspark.sql.functions import desc\n",
    "df.select('StockCode').where(\"StockCode in ('DOT', 'POST', 'C2')\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 INSTR - 특정 문자열이 포함되었는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|added|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|    8|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|    8|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\"\"\" instr 함수 \"\"\"\n",
    "df.withColumn(\"added\", instr(df.Description, \"POSTAGE\")).where(\"added > 1\").show() # 8번째 글자에 'POSTAGE'가 시작됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 수치형 데이터 타입 다루기\n",
    "### 3.1 각종 함수를 표현식으로 작성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "df.selectExpr(\"CustomerID\", \"pow(Quantity * UnitPrice, 2) + 5 as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 반올림(round), 올림(ceil), 버림(floor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+\n",
      "|round(2.5, 0)|CEIL(2.4)|FLOOR(2.6)|\n",
      "+-------------+---------+----------+\n",
      "|            3|        3|         2|\n",
      "+-------------+---------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.selectExpr(\"round(2.5, 0)\", \"ceil(2.4)\", \"floor(2.6)\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 요약 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|        InvoiceNo|\n",
      "+-------+-----------------+\n",
      "|  count|             3108|\n",
      "|   mean| 536516.684944841|\n",
      "| stddev|72.89447869788873|\n",
      "|    min|           536365|\n",
      "|    max|          C536548|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()\n",
    "df.describe(\"InvoiceNo\").show() # 컬럼을 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ StaFunctions 패키지의 통계 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문자열 데이터 타입 다루기\n",
    "### 4.1 첫 문자열만 대문자로 변경\n",
    "* 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경, initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|initcap(Description)              |\n",
      "+----------------------------------+\n",
      "|White Hanging Heart T-light Holder|\n",
      "|White Metal Lantern               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 대문자(upper), 소문자(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|  lower(Description)|  upper(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "df.selectExpr(\"Description\", \"lower(Description)\", \"upper(Description)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 문자열 주변의 공백을 제거, lpad/ltrim/rpad/rtrim/trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+---+----------+\n",
      "|   ltrim|   rtrim| trim| lp|        rp|\n",
      "+--------+--------+-----+---+----------+\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "+--------+--------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "    ltrim(lit(\"   HELLO   \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"   HELLO   \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"   HELLO   \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 정규 표현식\n",
    "### 5.1 단어 치환, regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|color_clean                       |Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|COLOR METAL LANTERN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GRENN|BLUE\"\n",
    "df.select(regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"), col(\"Description\")).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 날짜와 타임스팸프 데이터 타입 다루기\n",
    "> 시간대 설정이 필요하다면 스파크 SQL 설정의 spark.conf.sessionLocalTimeZone 속성으로 가능 <br>\n",
    "> TimestampType 클래스는 초 단위 정밀도만 지원 - 초 단위 이상 정밀도 요구 시 long 데이터 타입으로 데이터를 변환해 처리하는 우회 정책이 필요 <br>\n",
    "\n",
    "### 6.1 오늘 날짜 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+---+----------+--------------------------+\n",
      "|id |today     |now                       |\n",
      "+---+----------+--------------------------+\n",
      "|0  |2020-12-10|2020-12-10 00:33:35.180493|\n",
      "|1  |2020-12-10|2020-12-10 00:33:35.180493|\n",
      "|2  |2020-12-10|2020-12-10 00:33:35.180493|\n",
      "+---+----------+--------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10) \\\n",
    "    .withColumn(\"today\", current_date()) \\\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dataTable\")\n",
    "dateDF.printSchema()\n",
    "\n",
    "dateDF.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 날짜를 더하거나 빼기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2020-12-05|        2020-12-15|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_sub, date_add\n",
    "dateDF.select(\n",
    "    date_sub(col(\"today\"), 5),\n",
    "    date_add(col(\"today\"), 5)\n",
    ").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 문자열을 날짜로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "spark.range(5) \\\n",
    "    .withColumn(\"date\", lit(\"2017-01-01\")) \\\n",
    "    .select(to_date(col(\"date\"))) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 파싱오류로 날짜가 null로 반환되는 사례 \"\"\"\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1) # 월과 일의 순서가 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 널 값 다루기\n",
    "+ null 값을 사용하는 것 보다 명시적으로 사용하는 것이 항상 좋음\n",
    "+ null 값을 허용하지 않는 컬럼을 선언해도 강제성은 없음\n",
    "+ nullable 속성은 스파크 SQL 옵티마이저가 해당 컬럼을 제어하는 동작을 단순하게 돕는 역할\n",
    "+ null 값을 다루는 방법은 두 가지 \n",
    "    + 명시적으로 null을 제거\n",
    "    + 전역 또느 컬럼 단위로 null 값을 특정 값으로 채움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-1. 컬럼 값에 따른 널 처리 함수 (ifnull, nullIf, nvl, nvl2)\n",
    "+ SQL 함수이며 DataFrame의 select 표현식으로 사용 가능\n",
    "    + ifnull(null, 'return_value') # 두 번째 값을, 아니라면 첫 번째 값을 반환 \n",
    "    + nullif('value', 'value')     # 두 값이 같으면 null\n",
    "    + nvl(null, 'return_value')    # 두 번째 값을, 아니라면 첫 번째 값을 반환\n",
    "    + nvl2('not_null', 'return_value', 'else_value') # 두 번째 값을, 아니라면 세번째 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|ifnull(NULL, 'return_value')|nullif('value', 'value')|nvl(NULL, 'return_value')|nvl2('not null', 'return_value', 'else_value')|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|                return_value|                    null|             return_value|                                  return_value|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    ifnull(null, 'return_value'),\n",
    "    nullif('value', 'value'),\n",
    "    nvl(null, 'return_value'),\n",
    "    nvl2('not null', 'return_value', 'else_value')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2 컬럼의 널 값에 따른 로우 제거 (na.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.na.drop(\"any\").show(1) # 로우 컬럼값 중 하나라도 null이면 제거\n",
    "df.na.drop(\"all\").show(1) # 로우 컬럼값 모두 null이면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 배열 형태의 컬럼을 인수로 전달하여 지정한 컬럼만 제거합니다\n",
    "df.na.drop(\"all\", subset=(\"StockCode\", \"InvoiceNo\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 컬럼의 널 값에 따른 값을 채움 (na.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|   not_null|       World|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" null을 포함한 DataFrame 행성 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"string_null\", StringType(), True),\n",
    "    StructField(\"string2_null\", StringType(), True),\n",
    "    StructField(\"number_null\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "myRows = []\n",
    "myRows.append(Row(\"Hello\", None, float(5))) # string 컬럼에 null 포함\n",
    "myRows.append(Row(None, \"World\", None))     # number 컬럼에 null 포함\n",
    "\n",
    "myDf = spark.createDataFrame(myRows, myManualSchema)\n",
    "myDf.show()\n",
    "\n",
    "myDf.na.fill( {\"number_null\": 5.0, \"string_null\": \"not_null\"} ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습#5 data/tbl_purchase.csv 파일을 읽고, 상품명(p_name)을 소문자로 출력하세요\n",
    "> 참고: lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|lower(p_name)|\n",
      "+-------------+\n",
      "|      lg dios|\n",
      "|      lg gram|\n",
      "|      lg cyon|\n",
      "|        lg tv|\n",
      "|  lg computer|\n",
      "|      lg gram|\n",
      "|        lg tv|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchase = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/tbl_purchase.csv\")\n",
    "purchase.selectExpr(\"lower(p_name)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습#6 data/tbl_user.csv 파일을 읽고, \"now\" 라고 하는 현재 시간을 출력하는 컬럼을 추가해서 출력하세요\n",
    "> 참고: withColumn(\"column\", \"function\"), current_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+--------+--------------------------+\n",
      "|u_id|u_name    |u_gender|u_signup|now                       |\n",
      "+----+----------+--------+--------+--------------------------+\n",
      "|1   |정휘센    |남      |19580808|2020-12-10 00:33:43.966966|\n",
      "|2   |김싸이언  |남      |19590201|2020-12-10 00:33:43.966966|\n",
      "|3   |박트롬    |여      |19951030|2020-12-10 00:33:43.966966|\n",
      "|4   |청소기    |남      |19770329|2020-12-10 00:33:43.966966|\n",
      "|5   |유코드제로|여      |20021029|2020-12-10 00:33:43.966966|\n",
      "|6   |윤디오스  |남      |20040101|2020-12-10 00:33:43.966966|\n",
      "|7   |임모바일  |남      |20040807|2020-12-10 00:33:43.966966|\n",
      "|8   |조노트북  |여      |20161201|2020-12-10 00:33:43.966966|\n",
      "|9   |최컴퓨터  |남      |20201124|2020-12-10 00:33:43.966966|\n",
      "+----+----------+--------+--------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "user = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/tbl_user.csv\")\n",
    "user.withColumn(\"now\", current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
