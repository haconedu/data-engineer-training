{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|   key|   value|\n",
      "+------+--------+\n",
      "|   Cat|[28, 30]|\n",
      "|   Dog|[31, 28]|\n",
      "|Monkey| [3, 28]|\n",
      "|  Lion|[10, 24]|\n",
      "| Tiger| [6, 10]|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "animal = spark.createDataFrame([(\"Cat\", [28,30]), (\"Dog\", [31,28]), (\"Monkey\", [3,28]), (\"Lion\", [10,24]), (\"Tiger\", [6,10])], [\"Name\", \"Ages\"]).toDF(\"key\", \"value\")\n",
    "animal.createOrReplaceTempView(\"animal\")\n",
    "spark.sql(\"select * from animal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+------+-----+\n",
      "|   key|value|\n",
      "+------+-----+\n",
      "|   Cat|   28|\n",
      "|   Cat|   30|\n",
      "|   Dog|   31|\n",
      "|   Dog|   28|\n",
      "|Monkey|    3|\n",
      "|Monkey|   28|\n",
      "|   Cat|   10|\n",
      "|   Cat|   24|\n",
      "|   Dog|    6|\n",
      "|   Dog|   10|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg = animal.selectExpr(\"key\", \"EXPLODE(value) as value\")\n",
    "agg.printSchema()\n",
    "spark.sql(\"select * from agg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |      agg|       true|\n",
      "|        |   animal|       true|\n",
      "+--------+---------+-----------+\n",
      "\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     key|   string|   null|\n",
      "|   value|   bigint|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()\n",
    "spark.sql(\"describe agg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|   key|          values|\n",
      "+------+----------------+\n",
      "|   Cat|[28, 30, 10, 24]|\n",
      "|Monkey|         [3, 28]|\n",
      "|   Dog| [31, 28, 6, 10]|\n",
      "+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select key, collect_list(value) as values from agg group by key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|   key|  values|\n",
      "+------+--------+\n",
      "|   Cat|[29, 31]|\n",
      "|   Dog|[32, 29]|\n",
      "|  Lion|[11, 25]|\n",
      "|Monkey| [4, 29]|\n",
      "| Tiger| [7, 11]|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select key, collect_list(value + 1) as values from (select key, explode(value) as value from animal) as value group by key order by key asc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|   key|  values|\n",
      "+------+--------+\n",
      "|   Cat|[29, 31]|\n",
      "|   Dog|[32, 29]|\n",
      "|  Lion|[11, 25]|\n",
      "|Monkey| [4, 29]|\n",
      "| Tiger| [7, 11]|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType, StringType, LongType\n",
    "\n",
    "def plusOneInt(in_values):\n",
    "    out_values = []\n",
    "    for value in in_values:\n",
    "        out_values.append(value + 1)\n",
    "    return out_values\n",
    "\n",
    "spark.udf.register(\"plusOneInt\", plusOneInt, ArrayType(LongType()))\n",
    "spark.sql(\"select key, plusOneInt(value) AS values FROM animal order by key asc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\n",
      "*(2) Sort [key#1237 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(key#1237 ASC NULLS FIRST, 200), true, [id=#1078]\n",
      "   +- ObjectHashAggregate(keys=[key#1237], functions=[collect_list((value#1446L + 1), 0, 0)])\n",
      "      +- Exchange hashpartitioning(key#1237, 200), true, [id=#1076]\n",
      "         +- ObjectHashAggregate(keys=[key#1237], functions=[partial_collect_list((value#1446L + 1), 0, 0)])\n",
      "            +- Generate explode(value#1238), [key#1237], false, [value#1446L]\n",
      "               +- *(1) Project [Name#1233 AS key#1237, Ages#1234 AS value#1238]\n",
      "                  +- *(1) Scan ExistingRDD[Name#1233,Ages#1234]\n",
      "\n",
      "|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"explain select key, collect_list(value + 1) as values from (select key, explode(value) as value from animal) as value group by key order by key asc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\n",
      "*(3) Sort [key#1237 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(key#1237 ASC NULLS FIRST, 200), true, [id=#1110]\n",
      "   +- *(2) Project [Name#1233 AS key#1237, pythonUDF0#1463 AS values#1456]\n",
      "      +- BatchEvalPython [plusOneInt(Ages#1234)], [pythonUDF0#1463]\n",
      "         +- *(1) Scan ExistingRDD[Name#1233,Ages#1234]\n",
      "\n",
      "|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"explain select key, plusOneInt(value) AS values FROM animal order by key asc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
